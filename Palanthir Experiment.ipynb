{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Palanthir(object):\n",
    "\n",
    "    def __init__(self, input):\n",
    "        \"\"\"Initiates a Palanthir-class on-top a Pandas Dataframe. The class-attributes describes the overall structure and composition of the data\"\"\"\n",
    "        self.input_data = input\n",
    "        self.output = self.input_data.copy(deep=True)\n",
    "        self.observations = len(self.output)\n",
    "        self.features = list(self.output)\n",
    "        self.features_num = list(self.output.loc[:, self.output.dtypes != object])\n",
    "        self.features_cat = list(self.output.loc[:, self.output.dtypes == object])\n",
    "        self.train_subset = []\n",
    "        self.test_subset = []\n",
    "        self.transformation_history = []\n",
    "\n",
    "    def update_attributes(self, step=None):\n",
    "        self.observations = len(self.output)\n",
    "        self.features = list(self.output)\n",
    "        self.features_num = list(self.output.loc[:, self.output.dtypes != object])\n",
    "        self.features_cat = list(self.output.loc[:, self.output.dtypes == object])\n",
    "        self.transformation_history.append(step)\n",
    "\n",
    "    def summarize(self):\n",
    "        \"\"\"Prints the info, description and any missing value-counts for the class\"\"\"\n",
    "        dataset = self.output\n",
    "        return print(\n",
    "            \"Info: \", dataset.info(),\n",
    "            \"Description: \", dataset.describe(),\n",
    "            \"Missing values: \", dataset.isna().sum()\n",
    "        )\n",
    "\n",
    "    def random_split(self, test_size, store=True):\n",
    "        \"\"\"Uses the SKLearn Train_Test_Split to divide the dataset into random training and test subset\"\"\"\n",
    "        dataset = self.output\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train, test = train_test_split(dataset, test_size=test_size, random_state=42)\n",
    "        if store:\n",
    "            self.train_subset, self.test_subset = [train], [test]\n",
    "        return train, test\n",
    "\n",
    "    def stratified_split(self, cols, store=True):\n",
    "        \"\"\"Uses the SKLearn StratigiesShuffleSplit to divide the dataset into stratified training and test subset\"\"\"\n",
    "        dataset = self.output\n",
    "        from sklearn.model_selection import StratifiedShuffleSplit\n",
    "        split = StratifiedShuffleSplit(n_split=1, test_size=0.2, random_state=42)\n",
    "        for train_index, test_index in split.split(dataset, dataset[cols]):\n",
    "            strat_train_set = dataset.loc[train_index]\n",
    "            strat_test_set = dataset.loc[test_index]\n",
    "        if store:\n",
    "            self.train_subset, self.test_subset = [strat_train_set], [strat_test_set]\n",
    "        return strat_train_set, strat_test_set\n",
    "\n",
    "    def PCA(self, n_components=0.80, store=True):\n",
    "        dataset = self.output[self.features_num]\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca_data = PCA(n_components=n_components).fit_transform(dataset)\n",
    "        output_df = pd.DataFrame(pca_data, columns=[\"PCA_\" + str(col + 1) for col in range(pca_data.shape[1])],\n",
    "                                 index=dataset.index)\n",
    "        if store:\n",
    "            self.output = output_df\n",
    "            self.update_attributes(step=\"Performed Principal Component Analysis\")\n",
    "        explained_variance = PCA().fit(dataset).explained_variance_ratio_\n",
    "        cumsum = np.cumsum(explained_variance)\n",
    "        print(cumsum)\n",
    "        plt.plot([\"PCA\" + str(num) for num in range(1, len(cumsum) + 1)], cumsum)\n",
    "        plt.show()\n",
    "        return output_df\n",
    "\n",
    "    def fill_nulls(self, strategy=\"median\", store=True):\n",
    "        \"\"\"Uses the SKLearn SimpleImputer to fill out any missing values in the numerical features of the dataset\"\"\"\n",
    "        dataset = self.output[self.features_num]\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputed_data = SimpleImputer(strategy=strategy).fit_transform(dataset)\n",
    "        output_df = pd.DataFrame(imputed_data, columns=dataset.columns, index=dataset.index)\n",
    "        if store:\n",
    "            self.output[self.features_num] = output_df\n",
    "            self.update_attributes(step=\"Filled nulls\")\n",
    "        return output_df\n",
    "\n",
    "    def encode_order(self, store=True):\n",
    "        \"\"\"Uses the SKLearn OrdinalEncoder to order any categorical features of the dataset\"\"\"\n",
    "        dataset = self.output[self.features_cat]\n",
    "        from sklearn.preprocessing import OrdinalEncoder\n",
    "        encoded_data = OrdinalEncoder().fit_transform(dataset)\n",
    "        output_df = pd.DataFrame(encoded_data, columns=dataset.columns, index=dataset.index)\n",
    "        if store:\n",
    "            self.output[self.features_cat] = output_df\n",
    "            self.update_attributes(step=\"Encoded order of categorial features\")\n",
    "        return output_df\n",
    "\n",
    "    def make_dummies(self, store=True):\n",
    "        \"\"\"Uses the SKLearn OneHotEncoder to turn categorical features of the dataset into dummy-variables\"\"\"\n",
    "        dataset = self.output[self.features_cat]\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        encoder = OneHotEncoder().fit(dataset)\n",
    "        new_column_names = encoder.get_feature_names(dataset.columns)\n",
    "        dummy_data = encoder.transform(dataset).toarray()\n",
    "        dummy_data_df = pd.DataFrame(dummy_data, columns=[name for name in new_column_names], index=dataset.index)\n",
    "        output_df = pd.merge(self.output[self.features_num], dummy_data_df, left_index=True, right_index=True)\n",
    "        if store == True:\n",
    "            self.output = output_df\n",
    "            self.update_attributes(step=\"Turned categorical features into dummy variables\")\n",
    "        return output_df\n",
    "\n",
    "    def scale(self, store=True):\n",
    "        \"\"\"Uses the SKLearn StandardScaler to scall all numerical features of the dataset\"\"\"\n",
    "        dataset = self.output[self.features_num]\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        output_df = StandardScaler().fit_transform(dataset)\n",
    "        if store:\n",
    "            self.output[self.features_num] = output_df\n",
    "            self.update_attributes(step=\"Scaled feature-values\")\n",
    "        return output_df\n",
    "\n",
    "    def cluster(self, max_k=10, store=True):\n",
    "        \"\"\"Uses the SKLearn KMeans to cluster the dataset\"\"\"\n",
    "        dataset = self.output\n",
    "        from sklearn.cluster import KMeans\n",
    "        from matplotlib import pyplot\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(dataset) for k in range(1, max_k + 1)]\n",
    "        silhouettes = [silhouette_score(dataset, model.labels_) for model in kmeans_per_k[1:]]\n",
    "        best_k = silhouettes.index(max(silhouettes)) + 2\n",
    "        plt.plot(range(2, max_k + 1), silhouettes)\n",
    "        plt.xlabel(\"KMeans\")\n",
    "        plt.ylabel(\"Silhouette-score\")\n",
    "        plt.show()\n",
    "        print(\"Best silhouette is obtained with k as: \", best_k)\n",
    "        if store:\n",
    "            self.output[\"Cluster\"] = [\"Cluster \" + str(i) for i in\n",
    "                                      KMeans(n_clusters=best_k, random_state=42).fit_predict(dataset)]\n",
    "            self.update_attributes(step=\"Added Cluster-label as column to dataset\")\n",
    "        return self.output\n",
    "\n",
    "    def construct_pipeline(self):\n",
    "        \"\"\"Uses the SKLearn Pipeline and ColumnTransformer to construct a pipeline of transformations on the dataset - including filling out zeroes, scaling and dummifying\"\"\"\n",
    "        dataset = self.output\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        num_columns = []\n",
    "        cat_columns = []\n",
    "        num_pipeline = Pipeline([\n",
    "            (\"imputer\", self.fill_nulls()),\n",
    "            (\"scaler\", self.scale()),\n",
    "        ])\n",
    "        cat_pipeline = Pipeline([\n",
    "            (\"dummy\", self.make_dummies()),\n",
    "        ])\n",
    "        full_pipeline = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_columns),\n",
    "            (\"cat\", cat_pipeline, cat_columns),\n",
    "        ])\n",
    "        self.output = full_pipeline.fit_transform(dataset)\n",
    "        return self.output\n",
    "\n",
    "    def cross_validate(self, model, x, y, score_measure=\"neg_mean_squared_error\", folds=10):\n",
    "        \"\"\"Uses the SKLearn Cross_Val_Score to cross-validate one/several models on the training subset\"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(model, x, y, scoring=score_measure, cv=folds)\n",
    "        return scores\n",
    "\n",
    "    def full_analysis(self, model):\n",
    "        \"\"\"Conducts a full data-analysis pipeline on the dataset, including model training, evaluation and tuning\"\"\"\n",
    "        dataset = self.output\n",
    "        X_train, X_test, Y_train, Y_test = self.random_split(dataset)\n",
    "\n",
    "        sqrt_scores = np.sqrt(\n",
    "            -self.cross_validate(model, X_train, Y_train, score_measure=\"neg_mean_squared_error\", folds=10))\n",
    "        print(\n",
    "            \"RMSE-scores: \", sqrt_scores,\n",
    "            \"RMSE-mean: \", sqrt_scores.mean(),\n",
    "            \"RMSE-std: \", sqrt_scores.std()\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dir = 'C:/Users/JesperFrederiksen/PycharmProjects/ML-code/datasets/housing/housing.csv'\n",
    "df = pd.read_csv(dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pal = Palanthir(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['longitude',\n 'latitude',\n 'housing_median_age',\n 'total_rooms',\n 'total_bedrooms',\n 'population',\n 'households',\n 'median_income',\n 'median_house_value',\n 'ocean_proximity']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal.features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal.transformation_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['ocean_proximity']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal.features_cat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['longitude',\n 'latitude',\n 'housing_median_age',\n 'total_rooms',\n 'total_bedrooms',\n 'population',\n 'households',\n 'median_income',\n 'median_house_value']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal.scale()\n",
    "pal.features_num."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0  -1.327835  1.052548            0.982143    -0.804819       -0.970325   \n1  -1.322844  1.043185           -0.607019     2.045890        1.348276   \n2  -1.332827  1.038503            1.856182    -0.535746       -0.825561   \n3  -1.337818  1.038503            1.856182    -0.624215       -0.718768   \n4  -1.337818  1.038503            1.856182    -0.462404       -0.611974   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0   -0.974429   -0.977033       2.344766            2.129631        NEAR BAY  \n1    0.861439    1.669961       2.332238            1.314156        NEAR BAY  \n2   -0.820777   -0.843637       1.782699            1.258693        NEAR BAY  \n3   -0.766028   -0.733781       0.932968            1.165100        NEAR BAY  \n4   -0.759847   -0.629157      -0.012881            1.172900        NEAR BAY  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.327835</td>\n      <td>1.052548</td>\n      <td>0.982143</td>\n      <td>-0.804819</td>\n      <td>-0.970325</td>\n      <td>-0.974429</td>\n      <td>-0.977033</td>\n      <td>2.344766</td>\n      <td>2.129631</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.322844</td>\n      <td>1.043185</td>\n      <td>-0.607019</td>\n      <td>2.045890</td>\n      <td>1.348276</td>\n      <td>0.861439</td>\n      <td>1.669961</td>\n      <td>2.332238</td>\n      <td>1.314156</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.332827</td>\n      <td>1.038503</td>\n      <td>1.856182</td>\n      <td>-0.535746</td>\n      <td>-0.825561</td>\n      <td>-0.820777</td>\n      <td>-0.843637</td>\n      <td>1.782699</td>\n      <td>1.258693</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.337818</td>\n      <td>1.038503</td>\n      <td>1.856182</td>\n      <td>-0.624215</td>\n      <td>-0.718768</td>\n      <td>-0.766028</td>\n      <td>-0.733781</td>\n      <td>0.932968</td>\n      <td>1.165100</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.337818</td>\n      <td>1.038503</td>\n      <td>1.856182</td>\n      <td>-0.462404</td>\n      <td>-0.611974</td>\n      <td>-0.759847</td>\n      <td>-0.629157</td>\n      <td>-0.012881</td>\n      <td>1.172900</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pal.output.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mpal\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_order\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 90\u001B[0m, in \u001B[0;36mPalanthir.encode_order\u001B[1;34m(self, store)\u001B[0m\n\u001B[0;32m     88\u001B[0m output_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(encoded_data, columns\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mcolumns, index\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mindex)\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m store:\n\u001B[1;32m---> 90\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_cat] \u001B[38;5;241m=\u001B[39m \u001B[43men_data_df\u001B[49m\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_attributes(step\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncoded order of categorial features\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output_df\n",
      "\u001B[1;31mNameError\u001B[0m: name 'en_data_df' is not defined"
     ]
    }
   ],
   "source": [
    "pal.encode_order()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "0        NEAR BAY\n1        NEAR BAY\n2        NEAR BAY\n3        NEAR BAY\n4        NEAR BAY\n           ...   \n20635      INLAND\n20636      INLAND\n20637      INLAND\n20638      INLAND\n20639      INLAND\nName: ocean_proximity, Length: 20640, dtype: object"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
